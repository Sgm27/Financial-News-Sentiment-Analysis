{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ngoso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "manual_seed = 42\n",
    "torch.manual_seed(manual_seed)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import unidecode\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'all-data.csv'\n",
    "header = ['sentiment', 'content']\n",
    "df = pd.read_csv(\n",
    "    dataset_path,\n",
    "    names=header,\n",
    "    encoding='ISO-8859-1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral', 'negative', 'positive']\n"
     ]
    }
   ],
   "source": [
    "classes_list = df['sentiment'].unique()\n",
    "print(classes_list.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4841</th>\n",
       "      <td>1</td>\n",
       "      <td>LONDON MarketWatch -- Share prices ended lower...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842</th>\n",
       "      <td>0</td>\n",
       "      <td>Rinkuskiai 's beer sales fell by 6.5 per cent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4843</th>\n",
       "      <td>1</td>\n",
       "      <td>Operating profit fell to EUR 35.4 mn from EUR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844</th>\n",
       "      <td>1</td>\n",
       "      <td>Net sales of the Paper segment decreased to EU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4845</th>\n",
       "      <td>1</td>\n",
       "      <td>Sales in Finland decreased by 10.5 % in Januar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4846 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentiment                                            content\n",
       "0             0  According to Gran , the company has no plans t...\n",
       "1             0  Technopolis plans to develop in stages an area...\n",
       "2             1  The international electronic industry company ...\n",
       "3             2  With the new production plant the company woul...\n",
       "4             2  According to the company 's updated strategy f...\n",
       "...         ...                                                ...\n",
       "4841          1  LONDON MarketWatch -- Share prices ended lower...\n",
       "4842          0  Rinkuskiai 's beer sales fell by 6.5 per cent ...\n",
       "4843          1  Operating profit fell to EUR 35.4 mn from EUR ...\n",
       "4844          1  Net sales of the Paper segment decreased to EU...\n",
       "4845          1  Sales in Finland decreased by 10.5 % in Januar...\n",
       "\n",
       "[4846 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = {\n",
    "    class_name : idx for idx, class_name in enumerate(classes_list)\n",
    "}\n",
    "df['sentiment'] = df['sentiment'].apply(lambda x: classes[x])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def text_normalization(text):\n",
    "    text = text.lower()\n",
    "    text = unidecode.unidecode(text)\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    text = [word for word in text.split() if word not in english_stopwords]\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].apply(lambda x: text_normalization(x)).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for text in df['content']:\n",
    "    for word in text.split():\n",
    "        vocab.add(word)\n",
    "\n",
    "vocab.add('[PAD]')\n",
    "vocab.add('[UNK]')\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "def transform(text, word_to_idx, max_seq_len):\n",
    "    words = text.split()\n",
    "    words = words[:max_seq_len]\n",
    "    words = [word_to_idx.get(word, word_to_idx['[UNK]']) for word in words]\n",
    "    words += [word_to_idx['[PAD]']] * (max_seq_len - len(words))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size: 4846\n",
      "Train size: 3391\n",
      "Validation size: 970\n",
      "Test size: 485\n"
     ]
    }
   ],
   "source": [
    "val_size = 0.2\n",
    "test_size = 0.125\n",
    "is_shuffle = True\n",
    "\n",
    "texts = df['content'].tolist()\n",
    "labels = df['sentiment'].tolist()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    texts, labels, \n",
    "    test_size=val_size, \n",
    "    random_state=manual_seed, \n",
    "    shuffle=is_shuffle\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=test_size, \n",
    "    random_state=manual_seed, \n",
    "    shuffle=is_shuffle\n",
    ")\n",
    "\n",
    "print(f\"Total size: {len(texts)}\")\n",
    "print(f\"Train size: {len(X_train)}\")\n",
    "print(f\"Validation size: {len(X_val)}\")\n",
    "print(f\"Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialNewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, word_to_idx, max_seq_len, transform=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            text = self.transform(text, self.word_to_idx, self.max_seq_len)\n",
    "\n",
    "        text = torch.tensor(text)\n",
    "        return (text, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 32\n",
    "\n",
    "train_dataset = FinancialNewsDataset(\n",
    "    X_train, y_train,\n",
    "    word_to_idx, \n",
    "    max_seq_len, \n",
    "    transform= transform\n",
    ")\n",
    "\n",
    "val_dataset = FinancialNewsDataset(\n",
    "    X_val, y_val,\n",
    "    word_to_idx, \n",
    "    max_seq_len, \n",
    "    transform= transform\n",
    ")\n",
    "\n",
    "test_dataset = FinancialNewsDataset(\n",
    "    X_test, y_test,\n",
    "    word_to_idx, \n",
    "    max_seq_len, \n",
    "    transform= transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 64\n",
    "test_batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=test_batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=test_batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, num_classes, dropout_prob):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.rnn = nn.RNN(\n",
    "            embed_size, hidden_size,\n",
    "            num_layers=num_layers, \n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc1 = nn.Linear(hidden_size, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(16, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.norm(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(classes_list)\n",
    "embedding_dim = 64\n",
    "hidden_dim = 64\n",
    "num_layers = 2\n",
    "dropout_prob = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SentimentClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_size=embedding_dim,\n",
    "    hidden_size=hidden_dim,\n",
    "    num_layers=num_layers,\n",
    "    num_classes=num_classes,\n",
    "    dropout_prob=dropout_prob\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_losses.append(loss.item())\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return np.mean(val_losses), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, train_loader, val_loader, criterion, optimizer, device, epochs):\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        batch_train_loss = []\n",
    "\n",
    "        model.train()\n",
    "        for idx, (texts, labels) in enumerate(train_loader):\n",
    "            texts = texts.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(texts)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_train_loss.append(loss.item())\n",
    "\n",
    "        epoch_train_loss = np.mean(batch_train_loss)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "\n",
    "        val_losses, val_accuracy = evaluate(model, val_loader, criterion)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - Train Loss: {epoch_train_loss:.4f} - Val Loss: {val_losses:.4f} - Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500 - Train Loss: 0.7213 - Val Loss: 0.8524 - Val Accuracy: 0.6546\n",
      "Epoch 2/500 - Train Loss: 0.7098 - Val Loss: 0.8484 - Val Accuracy: 0.6536\n",
      "Epoch 3/500 - Train Loss: 0.7193 - Val Loss: 0.8427 - Val Accuracy: 0.6577\n",
      "Epoch 4/500 - Train Loss: 0.7083 - Val Loss: 0.8431 - Val Accuracy: 0.6526\n",
      "Epoch 5/500 - Train Loss: 0.7083 - Val Loss: 0.8425 - Val Accuracy: 0.6577\n",
      "Epoch 6/500 - Train Loss: 0.7046 - Val Loss: 0.8587 - Val Accuracy: 0.6526\n",
      "Epoch 7/500 - Train Loss: 0.7023 - Val Loss: 0.8388 - Val Accuracy: 0.6567\n",
      "Epoch 8/500 - Train Loss: 0.7158 - Val Loss: 0.8768 - Val Accuracy: 0.6464\n",
      "Epoch 9/500 - Train Loss: 0.6962 - Val Loss: 0.8588 - Val Accuracy: 0.6567\n",
      "Epoch 10/500 - Train Loss: 0.6987 - Val Loss: 0.8819 - Val Accuracy: 0.6412\n",
      "Epoch 11/500 - Train Loss: 0.6881 - Val Loss: 0.8558 - Val Accuracy: 0.6546\n",
      "Epoch 12/500 - Train Loss: 0.6871 - Val Loss: 0.8613 - Val Accuracy: 0.6567\n",
      "Epoch 13/500 - Train Loss: 0.6874 - Val Loss: 0.8550 - Val Accuracy: 0.6536\n",
      "Epoch 14/500 - Train Loss: 0.6888 - Val Loss: 0.8714 - Val Accuracy: 0.6515\n",
      "Epoch 15/500 - Train Loss: 0.6846 - Val Loss: 0.9199 - Val Accuracy: 0.6289\n",
      "Epoch 16/500 - Train Loss: 0.6907 - Val Loss: 0.8898 - Val Accuracy: 0.6474\n",
      "Epoch 17/500 - Train Loss: 0.6805 - Val Loss: 0.8656 - Val Accuracy: 0.6526\n",
      "Epoch 18/500 - Train Loss: 0.6800 - Val Loss: 0.8527 - Val Accuracy: 0.6588\n",
      "Epoch 19/500 - Train Loss: 0.6800 - Val Loss: 0.8933 - Val Accuracy: 0.6433\n",
      "Epoch 20/500 - Train Loss: 0.7128 - Val Loss: 0.8517 - Val Accuracy: 0.6505\n",
      "Epoch 21/500 - Train Loss: 0.6832 - Val Loss: 0.8524 - Val Accuracy: 0.6546\n",
      "Epoch 22/500 - Train Loss: 0.6770 - Val Loss: 0.8776 - Val Accuracy: 0.6495\n",
      "Epoch 23/500 - Train Loss: 0.6768 - Val Loss: 0.8706 - Val Accuracy: 0.6485\n",
      "Epoch 24/500 - Train Loss: 0.6905 - Val Loss: 0.8651 - Val Accuracy: 0.6557\n",
      "Epoch 25/500 - Train Loss: 0.6821 - Val Loss: 0.8480 - Val Accuracy: 0.6546\n",
      "Epoch 26/500 - Train Loss: 0.6818 - Val Loss: 0.8871 - Val Accuracy: 0.6433\n",
      "Epoch 27/500 - Train Loss: 0.6747 - Val Loss: 0.9106 - Val Accuracy: 0.6330\n",
      "Epoch 28/500 - Train Loss: 0.6679 - Val Loss: 0.8668 - Val Accuracy: 0.6505\n",
      "Epoch 29/500 - Train Loss: 0.6664 - Val Loss: 0.8845 - Val Accuracy: 0.6464\n",
      "Epoch 30/500 - Train Loss: 0.6675 - Val Loss: 0.8944 - Val Accuracy: 0.6464\n",
      "Epoch 31/500 - Train Loss: 0.6592 - Val Loss: 0.8562 - Val Accuracy: 0.6567\n",
      "Epoch 32/500 - Train Loss: 0.6660 - Val Loss: 0.8652 - Val Accuracy: 0.6567\n",
      "Epoch 33/500 - Train Loss: 0.6594 - Val Loss: 0.9153 - Val Accuracy: 0.6381\n",
      "Epoch 34/500 - Train Loss: 0.6548 - Val Loss: 0.9381 - Val Accuracy: 0.6309\n",
      "Epoch 35/500 - Train Loss: 0.6595 - Val Loss: 0.8567 - Val Accuracy: 0.6546\n",
      "Epoch 36/500 - Train Loss: 0.6535 - Val Loss: 0.8664 - Val Accuracy: 0.6567\n",
      "Epoch 37/500 - Train Loss: 0.6634 - Val Loss: 0.8842 - Val Accuracy: 0.6536\n",
      "Epoch 38/500 - Train Loss: 0.6485 - Val Loss: 0.9350 - Val Accuracy: 0.6361\n",
      "Epoch 39/500 - Train Loss: 0.6623 - Val Loss: 0.9254 - Val Accuracy: 0.6351\n",
      "Epoch 40/500 - Train Loss: 0.6496 - Val Loss: 0.9113 - Val Accuracy: 0.6392\n",
      "Epoch 41/500 - Train Loss: 0.6516 - Val Loss: 0.9182 - Val Accuracy: 0.6412\n",
      "Epoch 42/500 - Train Loss: 0.6421 - Val Loss: 0.8925 - Val Accuracy: 0.6464\n",
      "Epoch 43/500 - Train Loss: 0.6365 - Val Loss: 0.9012 - Val Accuracy: 0.6402\n",
      "Epoch 44/500 - Train Loss: 0.6354 - Val Loss: 0.9006 - Val Accuracy: 0.6443\n",
      "Epoch 45/500 - Train Loss: 0.6329 - Val Loss: 0.9018 - Val Accuracy: 0.6474\n",
      "Epoch 46/500 - Train Loss: 0.6338 - Val Loss: 0.9099 - Val Accuracy: 0.6536\n",
      "Epoch 47/500 - Train Loss: 0.6410 - Val Loss: 0.8936 - Val Accuracy: 0.6505\n",
      "Epoch 48/500 - Train Loss: 0.6287 - Val Loss: 0.9037 - Val Accuracy: 0.6454\n",
      "Epoch 49/500 - Train Loss: 0.6240 - Val Loss: 0.9073 - Val Accuracy: 0.6474\n",
      "Epoch 50/500 - Train Loss: 0.6278 - Val Loss: 0.8928 - Val Accuracy: 0.6505\n",
      "Epoch 51/500 - Train Loss: 0.6206 - Val Loss: 0.9104 - Val Accuracy: 0.6402\n",
      "Epoch 52/500 - Train Loss: 0.6242 - Val Loss: 0.9183 - Val Accuracy: 0.6247\n",
      "Epoch 53/500 - Train Loss: 0.6181 - Val Loss: 0.9089 - Val Accuracy: 0.6423\n",
      "Epoch 54/500 - Train Loss: 0.6385 - Val Loss: 0.8830 - Val Accuracy: 0.6619\n",
      "Epoch 55/500 - Train Loss: 0.6185 - Val Loss: 0.9022 - Val Accuracy: 0.6526\n",
      "Epoch 56/500 - Train Loss: 0.6092 - Val Loss: 0.8938 - Val Accuracy: 0.6485\n",
      "Epoch 57/500 - Train Loss: 0.6119 - Val Loss: 0.9177 - Val Accuracy: 0.6186\n",
      "Epoch 58/500 - Train Loss: 0.6041 - Val Loss: 0.9059 - Val Accuracy: 0.6258\n",
      "Epoch 59/500 - Train Loss: 0.6001 - Val Loss: 0.9126 - Val Accuracy: 0.6237\n",
      "Epoch 60/500 - Train Loss: 0.5997 - Val Loss: 0.9005 - Val Accuracy: 0.6443\n",
      "Epoch 61/500 - Train Loss: 0.6028 - Val Loss: 0.9040 - Val Accuracy: 0.6309\n",
      "Epoch 62/500 - Train Loss: 0.5937 - Val Loss: 0.9093 - Val Accuracy: 0.6340\n",
      "Epoch 63/500 - Train Loss: 0.5972 - Val Loss: 0.9152 - Val Accuracy: 0.6320\n",
      "Epoch 64/500 - Train Loss: 0.5866 - Val Loss: 0.9326 - Val Accuracy: 0.6165\n",
      "Epoch 65/500 - Train Loss: 0.5898 - Val Loss: 0.9414 - Val Accuracy: 0.6206\n",
      "Epoch 66/500 - Train Loss: 0.5797 - Val Loss: 0.9369 - Val Accuracy: 0.6186\n",
      "Epoch 67/500 - Train Loss: 0.5848 - Val Loss: 0.9249 - Val Accuracy: 0.6258\n",
      "Epoch 68/500 - Train Loss: 0.5834 - Val Loss: 0.9443 - Val Accuracy: 0.6216\n",
      "Epoch 69/500 - Train Loss: 0.5932 - Val Loss: 0.9034 - Val Accuracy: 0.6392\n",
      "Epoch 70/500 - Train Loss: 0.5855 - Val Loss: 0.9258 - Val Accuracy: 0.6268\n",
      "Epoch 71/500 - Train Loss: 0.5734 - Val Loss: 0.9263 - Val Accuracy: 0.6309\n",
      "Epoch 72/500 - Train Loss: 0.5695 - Val Loss: 0.9465 - Val Accuracy: 0.6227\n",
      "Epoch 73/500 - Train Loss: 0.5697 - Val Loss: 0.9261 - Val Accuracy: 0.6340\n",
      "Epoch 74/500 - Train Loss: 0.5726 - Val Loss: 0.9574 - Val Accuracy: 0.6216\n",
      "Epoch 75/500 - Train Loss: 0.5639 - Val Loss: 0.9500 - Val Accuracy: 0.6351\n",
      "Epoch 76/500 - Train Loss: 0.5614 - Val Loss: 0.9738 - Val Accuracy: 0.6155\n",
      "Epoch 77/500 - Train Loss: 0.5661 - Val Loss: 0.9652 - Val Accuracy: 0.6237\n",
      "Epoch 78/500 - Train Loss: 0.5588 - Val Loss: 0.9575 - Val Accuracy: 0.6309\n",
      "Epoch 79/500 - Train Loss: 0.5637 - Val Loss: 0.9687 - Val Accuracy: 0.6196\n",
      "Epoch 80/500 - Train Loss: 0.5528 - Val Loss: 0.9718 - Val Accuracy: 0.6237\n",
      "Epoch 81/500 - Train Loss: 0.5571 - Val Loss: 0.9901 - Val Accuracy: 0.6031\n",
      "Epoch 82/500 - Train Loss: 0.5479 - Val Loss: 0.9789 - Val Accuracy: 0.6216\n",
      "Epoch 83/500 - Train Loss: 0.5591 - Val Loss: 0.9981 - Val Accuracy: 0.6052\n",
      "Epoch 84/500 - Train Loss: 0.5403 - Val Loss: 0.9719 - Val Accuracy: 0.6247\n",
      "Epoch 85/500 - Train Loss: 0.5370 - Val Loss: 1.0058 - Val Accuracy: 0.6124\n",
      "Epoch 86/500 - Train Loss: 0.5335 - Val Loss: 0.9997 - Val Accuracy: 0.6186\n",
      "Epoch 87/500 - Train Loss: 0.5290 - Val Loss: 0.9725 - Val Accuracy: 0.6299\n",
      "Epoch 88/500 - Train Loss: 0.5332 - Val Loss: 1.0382 - Val Accuracy: 0.5979\n",
      "Epoch 89/500 - Train Loss: 0.5351 - Val Loss: 0.9986 - Val Accuracy: 0.6227\n",
      "Epoch 90/500 - Train Loss: 0.5309 - Val Loss: 1.0229 - Val Accuracy: 0.6227\n",
      "Epoch 91/500 - Train Loss: 0.5189 - Val Loss: 1.0309 - Val Accuracy: 0.6206\n",
      "Epoch 92/500 - Train Loss: 0.5117 - Val Loss: 1.0157 - Val Accuracy: 0.6196\n",
      "Epoch 93/500 - Train Loss: 0.5169 - Val Loss: 1.0397 - Val Accuracy: 0.6186\n",
      "Epoch 94/500 - Train Loss: 0.5113 - Val Loss: 1.0456 - Val Accuracy: 0.6206\n",
      "Epoch 95/500 - Train Loss: 0.5121 - Val Loss: 1.0338 - Val Accuracy: 0.6196\n",
      "Epoch 96/500 - Train Loss: 0.5058 - Val Loss: 1.0592 - Val Accuracy: 0.6072\n",
      "Epoch 97/500 - Train Loss: 0.5044 - Val Loss: 1.0535 - Val Accuracy: 0.6175\n",
      "Epoch 98/500 - Train Loss: 0.5006 - Val Loss: 1.0792 - Val Accuracy: 0.6062\n",
      "Epoch 99/500 - Train Loss: 0.4981 - Val Loss: 1.0778 - Val Accuracy: 0.6062\n",
      "Epoch 100/500 - Train Loss: 0.4990 - Val Loss: 1.0159 - Val Accuracy: 0.6330\n",
      "Epoch 101/500 - Train Loss: 0.4965 - Val Loss: 1.0751 - Val Accuracy: 0.6010\n",
      "Epoch 102/500 - Train Loss: 0.4935 - Val Loss: 1.0707 - Val Accuracy: 0.6072\n",
      "Epoch 103/500 - Train Loss: 0.4915 - Val Loss: 1.0755 - Val Accuracy: 0.6124\n",
      "Epoch 104/500 - Train Loss: 0.4923 - Val Loss: 1.0660 - Val Accuracy: 0.6175\n",
      "Epoch 105/500 - Train Loss: 0.4884 - Val Loss: 1.0749 - Val Accuracy: 0.6144\n",
      "Epoch 106/500 - Train Loss: 0.4822 - Val Loss: 1.0963 - Val Accuracy: 0.6082\n",
      "Epoch 107/500 - Train Loss: 0.4780 - Val Loss: 1.1093 - Val Accuracy: 0.6103\n",
      "Epoch 108/500 - Train Loss: 0.4871 - Val Loss: 1.1846 - Val Accuracy: 0.5979\n",
      "Epoch 109/500 - Train Loss: 0.4949 - Val Loss: 1.1036 - Val Accuracy: 0.6165\n",
      "Epoch 110/500 - Train Loss: 0.4820 - Val Loss: 1.1009 - Val Accuracy: 0.6165\n",
      "Epoch 111/500 - Train Loss: 0.4757 - Val Loss: 1.1086 - Val Accuracy: 0.6072\n",
      "Epoch 112/500 - Train Loss: 0.4630 - Val Loss: 1.1287 - Val Accuracy: 0.6113\n",
      "Epoch 113/500 - Train Loss: 0.4610 - Val Loss: 1.1192 - Val Accuracy: 0.6165\n",
      "Epoch 114/500 - Train Loss: 0.4580 - Val Loss: 1.1258 - Val Accuracy: 0.6247\n",
      "Epoch 115/500 - Train Loss: 0.4906 - Val Loss: 1.1261 - Val Accuracy: 0.6041\n",
      "Epoch 116/500 - Train Loss: 0.4585 - Val Loss: 1.1223 - Val Accuracy: 0.6103\n",
      "Epoch 117/500 - Train Loss: 0.4515 - Val Loss: 1.1540 - Val Accuracy: 0.6000\n",
      "Epoch 118/500 - Train Loss: 0.4537 - Val Loss: 1.2544 - Val Accuracy: 0.5866\n",
      "Epoch 119/500 - Train Loss: 0.4458 - Val Loss: 1.1622 - Val Accuracy: 0.6165\n",
      "Epoch 120/500 - Train Loss: 0.4491 - Val Loss: 1.1823 - Val Accuracy: 0.6072\n",
      "Epoch 121/500 - Train Loss: 0.4388 - Val Loss: 1.1430 - Val Accuracy: 0.6186\n",
      "Epoch 122/500 - Train Loss: 0.4334 - Val Loss: 1.1611 - Val Accuracy: 0.6124\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m criteria \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m----> 7\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriteria\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[17], line 20\u001b[0m, in \u001b[0;36mfit\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, device, epochs)\u001b[0m\n\u001b[0;32m     17\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     18\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 20\u001b[0m     batch_train_loss\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     22\u001b[0m epoch_train_loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(batch_train_loss)\n\u001b[0;32m     23\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(epoch_train_loss)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 1e-4\n",
    "epochs = 500\n",
    "\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train_losses, val_losses = fit(\n",
    "    model, train_loader, val_loader,\n",
    "    criteria, optimizer, device, epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.9099 - Test Accuracy: 0.6268\n",
      "Val Loss: 0.8484 - Val Accuracy: 0.6505\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = evaluate(model, test_loader, criteria)\n",
    "val_loss, val_accuracy = evaluate(model, val_loader, criteria)\n",
    "print(f\"Test Loss: {test_loss:.4f} - Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Val Loss: {val_loss:.4f} - Val Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
